<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Scene space</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo">Home</a>
									<ul class="icons">
										<li><a href="https://twitter.com/son_gaeun" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
										<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
									</ul>
								</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h1>Scene space</h1>
									</header>

                                    <!-- Content -->
                                    <h2 id="content">General direction</h2>
									<p>Our cognitive system represents real-world scenes in the high-dimensional representational space. This space consists of numerous psychological dimensions that correspond to various attributes of the scenes, such as colors, materials, layout, category, and many higher-level semantic features. My recent main research explores the nature of this representational space leveraging <i>Generative Adversarial Networks (GANs)</i>. The basic algorithm of GANs is to simulate the underlying probabilistic density distribution of a large dataset to generate fake data with comparable quality to the real data. An important point here is that the successfully learned distribution contains numerous dimensions similar to the psychological dimensions in human minds, and thus we can control the scene attributes using GANs as tightly as possible by sampling the vectors from its learned distribution. Currently, I am applying this technique to various psychophysics studies that were restricted to the non-realistic simple stimulus domain (e.g. orientations, colors, size of simple shapes) which was utilized only due to their high controllability. By expanding the psychophysics to the real-world scene domain, we will be able to achieve a better understanding of human behavior and cognition in ecologically valid settings. </p>

                                    <hr class="major" />

                                    <h2 id="content">Sub-projects</h2>
                                    <p><span class="image left"><video src="videos/wheel01_r08.mp4" alt="" autoplay loop>
                                    <h3>Scene Wheels</h3>
                                    </span>Precisely characterizing mental representations of visual experiences requires careful control of experimental stimuli. Recent work leveraging such stimulus control has led to important insights; however, these findings are constrained to simple visual properties like colour and line orientation. There remains a critical methodological barrier to characterizing perceptual and mnemonic representations of realistic visual experiences. Here, we introduce a novel method to systematically control visual properties of natural scene stimuli. Using generative adversarial networks (GAN), a state-of-art deep learning technique for creating highly realistic synthetic images, we generated scene wheels in which continuously changing visual properties smoothly transition between meaningful realistic scenes. To validate the efficacy of scene wheels, we conducted two behavioral experiments that assess perceptual and mnemonic representations attained from the scene wheels. In the perceptual validation experiment, we tested whether the continuous transition of scene images along the wheel is reflected in human perceptual similarity judgment. The perceived similarity of the scene images correspondingly decreased as distances between the images increase on the wheel. In the memory experiment, participants reconstructed to-be-remembered scenes from the scene wheels. Reconstruction errors for these scenes resemble error distributions observed in prior studies using simple stimulus properties. Importantly, perceptual similarity judgment and memory precision varied systematically with scene wheel radius. These findings suggest our novel approach offers a window into the mental representations of naturalistic visual experiences.</p>
                                    <li>Published at: <b>Son, G.</b>, Walther, D. B., & Mack, M. L. (2022). Scene wheels: Measuring perception and memory of real-world scenes with a continuous stimulus space. <i>Behavior Research Methods, 54</i>(1), 444-456.</li>
									<li>Data available at: <a href="https://osf.io/h5wpk/">https://osf.io/h5wpk/</a></li>

									<hr class="major" />
									
                                    <p><span class="image right"><img src="images/sceneCP.jpg" width="400px"; alt="" />
                                    <h3>Cateogory biases of real-world scene</h3>
                                    </span>In daily life, we experience complex visual environments in which numerous visual properties are tightly woven into holistic dimensions. Our visual system warps and compresses this visual input across its multiple stages of operations to arrive at perceptual insights that link to conceptual knowledge. Compelling demonstrations in object perception suggest high-level cognitive functions like categorization can impact how visual processing unfolds to, for example, distinctly biases or distort perception along category-relevant stimulus dimensions. However, whether or not such categorical perception mechanisms similarly impact the perception of real-world scenes remains an important open question. Here, we address this question in a novel learning task in which participants learned to categorize realistic scene images synthesized from an image space defined by continuously varying holistic visual properties. First, participants learned an arbitrary linear category boundary that divided scene space through feedback-based learning. Next, participants completed a visual working memory estimation task in which a target scene was briefly presented, then after a brief delay reconstructed from the continuous scene space. Memory reconstruction errors revealed systematic biases that tracked the subjective nature of each participant’s category learning. Specifically, errors were selectively biased along the diagnostic dimensions defined by participants’ acquired category boundaries. In other words, after only a short category learning session, scenes were remembered as being more similar to their respective learned categories at the expense of their veridical details. These results suggest that our visual system extracts diagnostic dimensions that optimize top-down task goals and actively leverages them for subsequent perception and memory. The highly complex and realistic nature of our stimulus space highlights the dynamic nature of visual perception and high-level cognition in an ecologically valid setting.</p>
                                    <li>Presented at: <b>Son, G.</b>, Walther, B. D., & Mack, M (2022). Category learning biases in real-world scene perception. <i>Talk presentation at the 22nd Annual Meeting of the Vision Sciences Society, St. Pete Beach, FL.</i></li>

                                    <hr class="major" />

                                    <p><span class="image left"><img src="images/hysteresis.jpg" width="400px"; alt="" />
                                    <h3>Scene Hysteresis</h3>
                                    </span>Perceptual categorization is a fundamental task in the human vision system. Although categorical judgements made by humans usually achieve high accuracy, many studies have suggested that human perception is dependent on perceptual history such that categorical judgements are influenced and not consistent in some situations.  Categorization on ambiguous stimuli is a good way of studying this phenomenon. Here we are interested in the categorization of ambiguous scene images in continuous transitions between scene categories. We created synthesized in-door scene images based on three categories: bedroom, living room and dining room. Participants were shown sequences of scene images that smoothly changed from one category to another and were asked to respond when they perceived a change in category. Each sequence was shown in two directions such that both transitions from category A to B and B to A were presented, and the differences in the categorical responses in the two directions were compared. Participants were also shown sequences of abrupt changes with sudden shifts between categories, and these responses were used to estimate response time. We predicted the perception of scene category  to be biased toward the starting category in a transition. Our results confirm this prediction: participants’ perception in the two directions for the same transition was different such that the perceived category change was delayed, and the ambiguous scene images tended to be categorized as the starting category of a transition, thus giving rise to a perceptual hysteresis. This study found evidence that the same stimuli can be interpreted in different ways for categorical judgement. During a dynamic change of stimuli, the human visual system tends to persist in the current interpretation when resolving ambiguities. This implies that in scene perception, instead of a tendency to shift the perception, a stable perception is preferred by the human visual system in dynamically changing environments.</p>
                                    <li>Presented at: Chen, H., <b>Son, G.</b> & Walther, B. D. (2022) Categorization of continuously changing ambiguous scenes. <i>Poster presentation at the 22nd Annual Meeting of the Vision Sciences Society, Virtual meeting.</i></li>



									
								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Search -->
								<section id="search" class="alt">
									<form method="post" action="#">
										<input type="text" name="query" id="query" placeholder="Search" />
									</form>
								</section>

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="about.html">About</a></li>							
										<li>
											<span class="opener">Projects</span>
											<ul>
												<li><a href="scene_space.html">Scene space</a></li>
												<li><a href="ensemble.html">Ensemble perception</a></li>
												<li><a href="vwm.html">Visual working memory</a></li>
												<li><a href="aesthetics.html">Aesthetics</a></li>
											</ul>
										</li>
										<li><a href="toolset.html">Tool sets</a></li>
									</ul>
								</nav>

							<!-- Section -->
                            <section>
                                <header class="major">
                                    <h2>Contact</h2>
                                </header>
                                <ul class="contact">
                                    <li class="icon solid fa-envelope"><a href="#">gaeun.son@mail.utoronto.ca</a></li>
                                </ul>
                            </section>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">&copy; Untitled. All rights reserved. Demo Images: <a href="https://unsplash.com">Unsplash</a>. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>